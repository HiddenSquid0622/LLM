{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbced43-e8a8-487e-96c2-96ea2cdfd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pdfminer.high_level import extract_text\n",
    "import docx\n",
    "from django.conf import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af485a-8dbe-4686-b1e7-a915884b0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = settings.GEMINI_API_KEY\n",
    "MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}\"\n",
    "EMBEDDING_MODEL_NAME = \"embedding-001\"\n",
    "EMBEDDING_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{EMBEDDING_MODEL_NAME}:embedContent?key={API_KEY}\"\n",
    "def call_gemini_generation_api(prompt):\n",
    "    \"\"\"Makes an API call to the Gemini text generation model.\"\"\"\n",
    "    if not API_URL:\n",
    "        print(\"Error: API_URL is not configured. Is the API key missing?\")\n",
    "        return None\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = json.dumps(payload).encode('utf-8')\n",
    "    req = urllib.request.Request(GENERATION_API_URL, data=data, headers=headers, method='POST')\n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.status == 200:\n",
    "                response_data = json.loads(response.read().decode('utf-8'))\n",
    "                return response_data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')\n",
    "            else:\n",
    "                print(f\"Error: Generation API call failed with status {response.status}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the generation API call: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410c9ad-b04f-43fc-9f97-12d868955ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generates a vector embedding for a given text using the embedding model.\"\"\"\n",
    "    if not EMBEDDING_API_URL:\n",
    "        print(\"Error: Embedding API_URL is not configured.\")\n",
    "        return None\n",
    "    payload = {\"model\": f\"models/{EMBEDDING_MODEL_NAME}\", \"content\": {\"parts\": [{\"text\": text}]}}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = json.dumps(payload).encode('utf-8')\n",
    "    req = urllib.request.Request(EMBEDDING_API_URL, data=data, headers=headers, method='POST')\n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.status == 200:\n",
    "                response_data = json.loads(response.read().decode('utf-8'))\n",
    "                return response_data.get('embedding', {}).get('value')\n",
    "            else:\n",
    "                print(f\"Error: Embedding API call failed with status {response.status}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the embedding API call: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c7c86-5a2d-4241-8b1e-79949d177c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_django_dataset(dataset):\n",
    "    processed_docs = {}\n",
    "    for doc_data in dataset:\n",
    "        file_name = doc_data.get(\"file_name\")\n",
    "        file_type = doc_data.get(\"file_type\")\n",
    "        encoded_content = doc_data.get(\"content\")\n",
    "        if not file_name or not file_type or not encoded_content:\n",
    "            print(f\"Missing fields in dataset item: {doc_data}\")\n",
    "            continue\n",
    "        try:\n",
    "            file_bytes = base64.b64decode(encoded_content)\n",
    "            content = \"\"\n",
    "            if file_type == 'pdf':\n",
    "                with io.BytesIO(file_bytes) as pdf_file:\n",
    "                    content = extract_text(pdf_file)\n",
    "                print(f\"Successfully decoded and processed PDF: {file_name}\")\n",
    "            elif file_type == 'docx':\n",
    "                with io.BytesIO(file_bytes) as docx_file:\n",
    "                    doc = docx.Document(docx_file)\n",
    "                    content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "                print(f\"Successfully decoded and processed DOCX: {file_name}\")\n",
    "            elif file_type in ['txt', 'email']:\n",
    "                content = file_bytes.decode('utf-8')\n",
    "                print(f\"Successfully decoded and processed Text/Email: {file_name}\")\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported file type '{file_type}' for {file_name}\")\n",
    "                continue\n",
    "            processed_docs[file_name] = content\n",
    "        except base64.binascii.Error:\n",
    "            print(f\"Error decoding base64 content for {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97d22f-31d8-4bf1-81f0-6662d08c60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query_with_llm(query):\n",
    "    prompt = f\"\"\"\n",
    "    You are an intelligent assistant. Your task is to parse the following user query and extract key details into a structured JSON format.\n",
    "    The possible keys are \"age\", \"gender\", \"procedure\", \"location\", and \"policy_duration_months\".\n",
    "    If a detail is not present in the query, omit the key from the JSON.\n",
    "    Your output must be only the JSON object, with no other text or markdown formatting.\n",
    "\n",
    "    User Query: \"{query}\"\n",
    "\n",
    "    JSON Output:\n",
    "    \"\"\"\n",
    "    print(\"\\nParsing Query with Gemini LLM\")\n",
    "    parsed_json_string = call_gemini_generationmapi(prompt)\n",
    "\n",
    "    if not parsed_json_string:\n",
    "        print(\"Error: Received no response from LLM for query parsing.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        cleaned_string = parsed_json_string.strip().replace('```json', '').replace('```', '').strip()\n",
    "        structured_query = json.loads(cleaned_string)\n",
    "        print(f\"Successfully parsed query into: {structured_query}\")\n",
    "        return structured_query\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: LLM did not return valid JSON. Response was: {parsed_json_string}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f2b48-f72c-4621-851a-99413100a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_with_embeddings(query, processed_docs, top_k=3):\n",
    "    print(\"\\n--- Performing Semantic Search with Vector Embeddings ---\")\n",
    "    if not processed_docs:\n",
    "        print(\"No document provided.\")\n",
    "        return []\n",
    "    doc_name, content = next(iter(processed_docs.items()))\n",
    "    paragraphs = [p.strip() for p in content.split('\\n') if len(p.strip()) > 50]\n",
    "    if not paragraphs:\n",
    "        print(\"No valid paragraphs found in the document.\")\n",
    "        return []\n",
    "    chunks = [{\"source\": doc_name, \"text\": p} for p in paragraphs]\n",
    "    print(f\"Created {len(chunks)} text chunks from document '{doc_name}'.\")\n",
    "    print(\"Generating embedding for the query...\")\n",
    "    query_embedding = get_embedding(query)\n",
    "    if not query_embedding:\n",
    "        print(\"Failed to generate embedding for the query.\")\n",
    "        return []\n",
    "    print(\"Generating embeddings for all document chunks...\")\n",
    "    chunk_embeddings = [get_embedding(chunk['text']) for chunk in chunks]\n",
    "    valid_embeddings_data = [\n",
    "        (emb, chunks[i]) for i, emb in enumerate(chunk_embeddings) if emb is not None\n",
    "    ]\n",
    "    if not valid_embeddings_data:\n",
    "        print(\"Failed to generate any embeddings for the document chunks.\")\n",
    "        return []\n",
    "    valid_embeddings, valid_chunks = zip(*valid_embeddings_data)\n",
    "    print(\"Calculating similarities...\")\n",
    "    query_vec = np.array(query_embedding).reshape(1, -1)\n",
    "    chunk_vecs = np.array(valid_embeddings)\n",
    "    similarities = cosine_similarity(query_vec, chunk_vecs)[0]\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    relevant_clauses = []\n",
    "    for index in top_k_indices:\n",
    "        clause = {\n",
    "            \"source\": valid_chunks[index][\"source\"],\n",
    "            \"clause\": valid_chunks[index][\"text\"],\n",
    "            \"similarity\": similarities[index]\n",
    "        }\n",
    "        relevant_clauses.append(clause)\n",
    "        print(f\"  - Found relevant clause from '{clause['source']}' (Similarity: {clause['similarity']:.4f}): \\\"{clause['clause'][:100]}...\\\"\")\n",
    "    return relevant_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd4278-294a-4f65-b368-a15abae09a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_decide_with_llm(structured_query, relevant_clauses):\n",
    "    query_str = json.dumps(structured_query, indent=2)\n",
    "    clauses_for_prompt = [{\"source\": c[\"source\"], \"clause\": c[\"clause\"]} for c in relevant_clauses]\n",
    "    clauses_str = \"\\n\\n\".join([f\"Source: {c['source']}\\nClause: {c['clause']}\" for c in clauses_for_prompt])\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert insurance claims processor. Based on the user's details and the provided policy clauses, make a decision.\n",
    "    Your response MUST be a single, raw JSON object with three keys: \"decision\" (string: \"Approved\", \"Rejected\", or \"Further Information Required\"),\n",
    "    \"amount\" (integer, the approved amount if applicable, otherwise 0), and \"justification\" (string, explaining the reason and referencing the source and clause).\n",
    "    Do not include any other text or markdown formatting.\n",
    "\n",
    "    User Details:\n",
    "    {query_str}\n",
    "\n",
    "    Relevant Policy Clauses:\n",
    "    {clauses_str}\n",
    "\n",
    "    Decision JSON:\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Clauses with Gemini LLM for Final Decision ---\")\n",
    "    decision_json_string = call_gemini_generation_api(prompt)\n",
    "    if not decision_json_string:\n",
    "        return {\"decision\": \"Error\", \"amount\": 0, \"justification\": \"Received no response from LLM.\"}\n",
    "    try:\n",
    "        cleaned_string = decision_json_string.strip().replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(cleaned_string)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: LLM did not return valid JSON for the decision. Response was: {decision_json_string}\")\n",
    "        return {\"decision\": \"Error\", \"amount\": 0, \"justification\": \"Failed to parse decision from LLM.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950def1-1db6-408d-95c2-b69c37873451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_pipeline(query, django_dataset):\n",
    "    if not API_KEY:\n",
    "        return {\"error\": \"Gemini API key is not configured. Please check.\"}\n",
    "    processed_docs = load_documents_from_django_dataset(django_dataset)\n",
    "    if not processed_docs: return {\"error\": \"No documents could be loaded or processed.\"}\n",
    "    structured_query = parse_query_with_llm(query)\n",
    "    if not structured_query: return {\"error\": \"Failed to parse the user query.\"}\n",
    "    relevant_clauses = semantic_search_with_embeddings(query, processed_docs)\n",
    "    if not relevant_clauses:\n",
    "        return {\"decision\": \"Cannot Determine\", \"amount\": 0, \"justification\": \"No relevant clauses found for the query.\"}\n",
    "    final_response = evaluate_and_decide_with_llm(structured_query, relevant_clauses)\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e6333-5f29-4256-a093-0fbbfbf0e728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
